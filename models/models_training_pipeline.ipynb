{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:14:53.468043600Z",
     "start_time": "2024-04-11T08:14:53.385559800Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from service.s3_storage_service import S3StorageService\n",
    "from storage.s3_client import S3Client\n",
    "\n",
    "load_dotenv() # Load environment variable with the bucket name. \n",
    "s3_client = S3Client().get_client()\n",
    "s3_service = S3StorageService(s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define functions for data retrieval and storage in a dataframe\n",
    "#### First, retrieve the data from each csv link stored in the station's folder.\n",
    "#### If the last entry of retrieved dataset is not the same as current date, stop the downloading process completely"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4487df44c5497ce4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def try_parsing_date(text):\n",
    "    for fmt in ('%d/%m/%Y', '%Y-%m-%d'):  # Add or remove formats as needed\n",
    "        try:\n",
    "            return pd.to_datetime(text, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return parse(text, dayfirst=True)  # Fallback on dateutil's parse\n",
    "\n",
    "def preprocess_data(df, parameter_name, fill_method='mean'):\n",
    "    # First check if the last date is up to the current date\n",
    "    if df.index.max().date() != datetime.now().date():\n",
    "        print(f\"Data for {parameter_name} is not up to the current date: {df.index.max().date()}\")\n",
    "        return None  # Stop processing this data\n",
    "\n",
    "    # Ensure all dates in the range are present\n",
    "    all_dates = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "    df = df.reindex(all_dates)\n",
    "\n",
    "    # Fill NaN values in the column based on the parameter\n",
    "    if parameter_name == 'rain':\n",
    "        # For rain data, fill gaps with 0\n",
    "        df[parameter_name] = df[parameter_name].fillna(0)\n",
    "    else:\n",
    "        # For other types of data, fill gaps with the mean or specified method\n",
    "        if fill_method == 'mean':\n",
    "            df[parameter_name] = df[parameter_name].fillna(df[parameter_name].mean())\n",
    "        else:\n",
    "            print(f\"Unknown fill method for {parameter_name}.\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def verify_required_columns(df, interval, required_columns):\n",
    "    if interval == '15min':\n",
    "        # Adjust required columns for 15min interval data\n",
    "        required_columns = ['finst', 'linst']  # Adjust as needed, rain is optional\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Missing required columns: {missing_columns} for interval {interval}\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv_data_to_dataframe(data):\n",
    "    dfs = []\n",
    "    process_failed = False  # Flag to check if any dataset preprocessing failed\n",
    "    \n",
    "    for item in data:\n",
    "        if process_failed:  # Check if the flag is already set to True\n",
    "            print(\"Process stopped due to outdated dataset.\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame\n",
    "        \n",
    "        url = item['link']\n",
    "        parameter_name = item['type']\n",
    "        \n",
    "        df = pd.read_csv(url, usecols=['date', 'value'])\n",
    "        df.rename(columns={'value': parameter_name}, inplace=True)\n",
    "        df['date'] = df['date'].apply(try_parsing_date)\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "        # Preprocess each dataset\n",
    "        preprocessed_df = preprocess_data(df, parameter_name)\n",
    "        if preprocessed_df is None:\n",
    "            process_failed = True  # Set flag to True if preprocessing failed\n",
    "        else:\n",
    "            dfs.append(preprocessed_df)\n",
    "\n",
    "    if process_failed:  # Additional check, in case the loop was skipped\n",
    "        print(\"Process stopped due to outdated dataset.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all preprocessed dataframes\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.merge(df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    combined_df.sort_values(by='date', inplace=True)\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_csv_urls(station_guid, measurement_interval=\"1d\"):\n",
    "    if measurement_interval == \"15min\":\n",
    "        filename = \"15min_measurements_links.json\"\n",
    "    elif measurement_interval == \"1d\":\n",
    "        filename = \"1d_measurements_links.json\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid measurement interval. Choose either '15min' or '1d'.\")\n",
    "    \n",
    "    return s3_service.load_json_from_s3(f\"flood_stations/{station_guid}/{filename}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:08.699639500Z",
     "start_time": "2024-04-11T08:15:08.690554900Z"
    }
   },
   "id": "d7e7c2da6b8be824",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "example_id = '00dfeaa4-34bd-4975-967a-b11e5d86520e'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:09.998672600Z",
     "start_time": "2024-04-11T08:15:09.994673Z"
    }
   },
   "id": "db958f9278c49906",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for fmean is not up to the current date: 2024-04-09\n",
      "Process stopped due to outdated dataset.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "urls = generate_csv_urls(example_id)\n",
    "df = load_csv_data_to_dataframe(urls)\n",
    "print(df.head())\n",
    "print(df.tail(100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:14.248628600Z",
     "start_time": "2024-04-11T08:15:10.957116Z"
    }
   },
   "id": "343051d62662ceca",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Produce two dataframes: one with rain column and one without. This is because the rainfall has been measured for a considerably shorter time, therefore resulting dataset would have much less data. The idea is to have two datasets that are used to train two separate models, later used in ensemble."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ceb9a427a0ac98fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def split_dataframe_by_rain(df):\n",
    "    # Check if 'rain' column exists in the DataFrame\n",
    "    if 'rain' in df.columns:\n",
    "        # DataFrame without 'rain' column\n",
    "        df_without_rain = df.drop(columns=['rain'])\n",
    "        # DataFrame only with 'rain' column and the index\n",
    "        df_with_rain = df[['rain']]\n",
    "    else:\n",
    "        # If there's no 'rain' column, return the original df and an empty df for rain\n",
    "        df_without_rain = df.copy()\n",
    "        df_with_rain = pd.DataFrame(columns=['rain'], index=df.index)\n",
    "    \n",
    "    return df_without_rain, df_with_rain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:14.248628600Z",
     "start_time": "2024-04-11T08:15:14.245566300Z"
    }
   },
   "id": "195069c9799c7b1b",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trim the dataframe so it does not have any NaN values at the start. It is because certain values have been recorded much earlier than the others. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f26e0ab0f054afb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def trim_dataframe_start(df):\n",
    "    # Find the index of the first non-NaN value in each column\n",
    "    first_valid_indices = df.apply(pd.Series.first_valid_index)\n",
    "    \n",
    "    # Find the maximum of these indices to get the latest start point across all columns\n",
    "    latest_start_index = max(first_valid_indices)\n",
    "    \n",
    "    # Trim the DataFrame to start from this index\n",
    "    trimmed_df = df.loc[latest_start_index:]\n",
    "    \n",
    "    return trimmed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:15:15.778763900Z",
     "start_time": "2024-04-11T08:15:15.775763100Z"
    }
   },
   "id": "ab21afd4a95c2062",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalise all values in the dataframe to prepare them for the training process. Keep the scaler for later use."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a06ec98ebf0b45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def prepare_data_for_training(df, n_input, n_output, batch_size, target_column):\n",
    "    \"\"\"\n",
    "    Prepares data for training by normalizing and creating TimeseriesGenerators.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the time series data.\n",
    "        n_input (int): Number of input time steps to use for each output.\n",
    "        n_output (int): Number of output time steps.\n",
    "        batch_size (int): Batch size for the generators.\n",
    "        target_column (str): The name of the target column for prediction.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing train_generator, test_generator, scaler, n_features,\n",
    "               and additional info like split index and column target index.\n",
    "    \"\"\"\n",
    "    # Normalize the data\n",
    "    data = df.values\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    split_percent = 0.95\n",
    "    split_index = int(len(data_normalized) * split_percent)\n",
    "    train_data = data_normalized[:split_index]\n",
    "    test_data = data_normalized[split_index - n_input:]\n",
    "\n",
    "    # n_features is the number of columns in the dataframe\n",
    "    n_features = df.shape[1]\n",
    "\n",
    "    # Determine the index of the target column\n",
    "    target_index = df.columns.get_loc(target_column)\n",
    "\n",
    "    # Create TimeseriesGenerators\n",
    "    train_generator = TimeseriesGenerator(train_data, train_data[:, target_index],\n",
    "                                          length=n_input, batch_size=batch_size)\n",
    "    test_generator = TimeseriesGenerator(test_data, test_data[:, target_index],\n",
    "                                         length=n_input, batch_size=batch_size)\n",
    "\n",
    "    return train_generator, test_generator, scaler, n_features, split_index, target_index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8ba3c474de9fb2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, test_generator, scaler, n_output):\n",
    "    # Generate predictions for the test data\n",
    "    predictions = model.predict(test_generator)\n",
    "    predictions_reshaped = predictions.reshape(-1, 1)\n",
    "\n",
    "    # Collect the actual y values from the test generator\n",
    "    actuals = []\n",
    "    for _, y in test_generator:\n",
    "        actuals.extend(y)\n",
    "    y_test = np.array(actuals)\n",
    "    y_test_reshaped = y_test.reshape(-1, 1)\n",
    "\n",
    "    # Inverse transform predictions and actuals to original scale\n",
    "    predictions_inverse = scaler.inverse_transform(predictions_reshaped).flatten()\n",
    "    actuals_inverse = scaler.inverse_transform(y_test_reshaped).flatten()\n",
    "\n",
    "    # Initialize an empty list to store RMSE for each prediction window\n",
    "    rmse_values = []\n",
    "\n",
    "    # Loop through the dataset, aligning each prediction with its corresponding actual values\n",
    "    for i in range(len(actuals_inverse) - n_output + 1):  # Adjusted to avoid index error\n",
    "        # Extract the actual values and predictions for the window\n",
    "        actuals_for_window = actuals_inverse[i:i+n_output]\n",
    "        predictions_for_window = predictions_inverse[i:i+n_output]  # Adjusted to correctly extract predictions\n",
    "        \n",
    "        # Calculate the RMSE for this window and append to our list\n",
    "        rmse = sqrt(mean_squared_error(actuals_for_window, predictions_for_window))\n",
    "        rmse_values.append(rmse)\n",
    "\n",
    "    # Calculate the average RMSE across all windows for an overall performance metric\n",
    "    average_rmse = np.mean(rmse_values)\n",
    "\n",
    "    return average_rmse\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T10:07:10.842543Z",
     "start_time": "2024-04-11T10:07:10.797726500Z"
    }
   },
   "id": "e00dc60d755303ae",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train_model(train_generator, test_generator, n_input, n_features, n_output, scaler, model_type='LSTM', learning_rate=0.001, epochs=10, patience=10, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Trains an LSTM or GRU model based on the given parameters and data generators.\n",
    "\n",
    "    Parameters:\n",
    "    - train_generator: The generator providing training data.\n",
    "    - val_generator: The generator providing validation data.\n",
    "    - n_input: Number of input time steps.\n",
    "    - n_features: Number of features.\n",
    "    - n_output: Number of output time steps.\n",
    "    - model_type: Type of model to train ('LSTM' or 'GRU').\n",
    "    - learning_rate: Learning rate for the optimizer.\n",
    "    - epochs: Number of epochs to train for.\n",
    "    - patience: Number of epochs with no improvement after which training will be stopped.\n",
    "    - dropout_rate: Dropout rate for regularization.\n",
    "    - model_save_path: Path to save the trained model.\n",
    "    - performance_save_path: Path to save the model's performance statistics.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing training history and model performance statistics.\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(128, return_sequences=False, input_shape=(n_input, n_features)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(128, return_sequences=False, input_shape=(n_input, n_features)))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Choose 'LSTM' or 'GRU'.\")\n",
    "    \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_output))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(delta=0.005))\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_generator, validation_data=test_generator, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "    # Prepare performance statistics\n",
    "    performance_stats = {\n",
    "        'history': history.history,\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        'final_val_accuracy': history.history['val_accuracy'][-1] if 'val_accuracy' in history.history else None,\n",
    "        'average_rmse': evaluate_model_performance(model,test_generator,scaler, n_output)\n",
    "    }\n",
    "\n",
    "\n",
    "    return performance_stats, model, model_type"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:54:02.425884100Z",
     "start_time": "2024-04-11T11:54:02.420954700Z"
    }
   },
   "id": "144bc92988e57430",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_metadata(performance_stats, n_input, n_output, contains_rainfall, model_type, measurement_interval, station_guid, target_column ):\n",
    "    metadata = {\n",
    "        'station_guid' : station_guid,\n",
    "        'n_input': n_input,\n",
    "        'n_output': n_output,\n",
    "        'measurement_interval' : measurement_interval,\n",
    "        'rainfall': contains_rainfall,\n",
    "        'model_type' : model_type,\n",
    "        'average_rmse' : performance_stats['average_rmse'],\n",
    "        'target_column': target_column\n",
    "    }\n",
    "    return metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T12:02:48.307898Z",
     "start_time": "2024-04-11T12:02:48.295323800Z"
    }
   },
   "id": "a434ab2af5318e58",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tensorflow.keras import backend as K\n",
    "import shutil\n",
    "\n",
    "def generate_model_path(metadata):\n",
    "    rainfall = 'rainfall' if metadata['rainfall'] else 'no_rainfall'\n",
    "    return f\"flood_stations/{metadata['station_guid']}/models/{metadata['measurement_interval']}_{metadata['model_type']}_{rainfall}/\"\n",
    "        \n",
    "\n",
    "def save_to_s3_and_cleanup(model, scaler, performance_stats, metadata, model_id):\n",
    "    \"\"\"\n",
    "    Saves the model, scaler, performance stats, and metadata to S3, then cleans up local files.\n",
    "\n",
    "    Parameters:\n",
    "    - s3_service (S3StorageService): Instance of the S3StorageService class.\n",
    "    - model (tf.keras.Model): Trained model to save.\n",
    "    - scaler (sklearn.preprocessing): Scaler object used for the model.\n",
    "    - performance_stats (dict): Performance statistics of the model.\n",
    "    - metadata (dict): Metadata about the model training.\n",
    "    - model_id (str): Unique identifier for the model.\n",
    "    \"\"\"\n",
    "    model_storage_key = generate_model_path(metadata)\n",
    "    # Save Model\n",
    "    model_dir = f\"tmp/{model_id}\"\n",
    "    model.save(model_dir)\n",
    "    \n",
    "    s3_service.save_file_to_s3(model_storage_key, model_dir)\n",
    "    shutil.rmtree(model_dir)  # Clean up the local directory\n",
    "\n",
    "    # Save Scaler\n",
    "    scaler_path = f\"tmp/{model_id}_scaler.save\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    s3_service.save_file_to_s3(model_storage_key, scaler_path)\n",
    "    os.remove(scaler_path)  # Remove the local file\n",
    "\n",
    "    # Save Performance Metrics\n",
    "    s3_service.save_json_to_s3(f\"{model_storage_key}/performance_metrics.json\", performance_stats)\n",
    "\n",
    "    # Save Metadata\n",
    "    s3_service.save_json_to_s3(f\"{model_storage_key }/metadata.json\", metadata)\n",
    "    \n",
    "    K.clear_session()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cab144dc7f61fc0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_station(guid):\n",
    "    required_columns = ['lmax', 'lmin', 'fmax', 'fmin', 'fmean']\n",
    "\n",
    "    results = []\n",
    "    intervals = {\n",
    "        '1d': ['lmax', 'lmin'],\n",
    "        '15min': ['finst', 'linst']  # Assuming these are the correct identifiers\n",
    "    }\n",
    "\n",
    "    for interval, targets in intervals.items():\n",
    "        urls = generate_csv_urls(guid, interval)\n",
    "        df = load_csv_data_to_dataframe(urls)\n",
    "        if df is not None:\n",
    "            df = verify_required_columns(df, interval, required_columns)\n",
    "            if df is not None:\n",
    "                df_without_rain, df_with_rain = split_dataframe_by_rain(df)\n",
    "                for target_column in targets:\n",
    "                    # Process both with and without rain\n",
    "                    if not df_without_rain.empty:\n",
    "                        results.append(process_data_model(df_without_rain, interval, target_column, guid, False))\n",
    "                    if not df_with_rain.empty:\n",
    "                        results.append(process_data_model(df_with_rain, interval, target_column, guid, True))\n",
    "            else:\n",
    "                results.append((guid, interval, \"Missing required columns\"))\n",
    "        else:\n",
    "            results.append((guid, interval, \"No data available\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data_model(df, interval, target_column, guid, contains_rainfall):\n",
    "    # Trim the data\n",
    "    trimmed_df = trim_dataframe_start(df)\n",
    "\n",
    "    # Define parameters based on the interval type\n",
    "    n_input, n_output, model_type = (64, 7, 'LSTM') if interval == '1d' else (96, 24, 'LSTM')\n",
    "\n",
    "    # Prepare and train models\n",
    "    train_gen, test_gen, scaler, n_features, split_idx, target_idx = prepare_data_for_training(\n",
    "        trimmed_df, n_input, n_output, batch_size=32, target_column=target_column\n",
    "    )\n",
    "    performance_stats, model, model_type = train_model(\n",
    "        train_gen, test_gen, n_input, n_features, n_output, scaler,\n",
    "        model_type=model_type, learning_rate=0.001, epochs=10, patience=10, dropout_rate=0.2\n",
    "    )\n",
    "    metadata = generate_metadata(performance_stats, n_input, n_output, contains_rainfall, model_type, interval, guid, target_column)\n",
    "    save_to_s3_and_cleanup(model, scaler, performance_stats, metadata, guid)\n",
    "    return (guid, interval, target_column, 'Model trained and saved', contains_rainfall)\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T23:58:38.535730900Z",
     "start_time": "2024-04-11T23:58:38.523530800Z"
    }
   },
   "id": "1c043e11741aa3d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_last_processed_guid():\n",
    "    try:\n",
    "        # Assume you have a function to load this from S3\n",
    "        data = s3_service.load_json_from_s3(\"last_processed_guid.json\")\n",
    "        return data.get('last_guid', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load last processed GUID: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_last_processed_guid(guid):\n",
    "    try:\n",
    "        # Save to S3\n",
    "        s3_service.save_json_to_s3(\"last_processed_guid.json\", {'last_guid': guid})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save last processed GUID: {e}\")\n",
    "\n",
    "def process_stations(stations):\n",
    "    last_processed_guid = get_last_processed_guid()\n",
    "    start_processing = False if last_processed_guid else True\n",
    "\n",
    "    for station in stations:\n",
    "        guid = station['guid']\n",
    "        if not start_processing:\n",
    "            if guid == last_processed_guid:\n",
    "                start_processing = True\n",
    "            continue  # Skip until we reach the last processed guid\n",
    "\n",
    "        try:\n",
    "            # Here you would call your function to process each station\n",
    "            print(f\"Processing station with GUID: {guid}\")\n",
    "            # Assuming a function to handle all steps:\n",
    "            result = process_station(guid)\n",
    "            save_last_processed_guid(guid)  # Update the last successfully processed guid\n",
    "\n",
    "            if result is None:\n",
    "                print(f\"No data available for station {guid}, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing station {guid}: {e}\")\n",
    "            continue  # Continue with next station even in case of error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stations = [\n",
    "        # Your list of stations goes here\n",
    "    ]\n",
    "    process_stations(stations)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cab742bf0116335"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
